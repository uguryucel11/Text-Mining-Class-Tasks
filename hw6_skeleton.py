# -*- coding: utf-8 -*-
"""hw6_skeleton.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hMbboOFhFvIgIfxaWn13tJeu-QoBUc5X
"""

#Download and unzip files
!pip3 install scikit-learn
!wget http://computational-linguistics-class.org/homework/nn-lms/cities_test.txt
!wget http://computational-linguistics-class.org/homework/nn-lms/cities_val.zip
!wget http://computational-linguistics-class.org/homework/nn-lms/cities_train.zip
!sudo apt-get install unzip
!unzip cities_val.zip 
!unzip cities_train.zip 
from os.path import exists
!pip install "wheel==0.34.2"
from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag
platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())
cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\.\([0-9]*\)\.\([0-9]*\)$/cu\1\2/'
accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'
!pip3 install https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl
!pip3 install torch torchvision
  
import torch
device =  torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

#Verfiy file download
!head train/af.txt
!printf "\n"
!head val/af.txt
!printf "\n"
!head cities_test.txt
!printf "\n"
#Verify CUDA acceleration should print cuda:0
print(device)

# Mount your google drive. 
# Use this to save your PyTorch model for submission
from google.colab import drive
drive.mount('/content/gdrive')
!mkdir /content/gdrive/My\ Drive/cis530_hw6
#Test drive access. 
#You should have a test.txt under a new folder cis530_hw6 in your Google drive
with open('/content/gdrive/My Drive/cis530_hw6/test.txt', 'w') as f:
  f.write('This is a test!')

#main_classify.py
import codecs
import math
import random
import string
import time
import numpy as np
import torch
from sklearn.metrics import accuracy_score
import tqdm.notebook as tq

'''
Don't change these constants for the classification task.
You may use different copies for the sentence generation model.
'''
languages = ["af", "cn", "de", "fi", "fr", "in", "ir", "pk", "za"]
all_letters = string.ascii_letters + " .,;'"

'''
Returns the words of the language specified by reading it from the data folder
Returns the validation data if train is false and the train data otherwise.
Return: A nx1 array containing the words of the specified language
'''
def getWords(baseDir = '/content/gdrive/My Drive/cis530_hw6/', lang = 'de', train = True):
  result = []
  path = baseDir + 'train/' if train else baseDir + 'val/'

  with codecs.open(path + lang + '.txt', 'r', encoding='utf-8', errors='ignore') as f:
    for line in f:
      result.append(line[:-1])

  return np.array(result)
# print(getWords()[:10])

'''
Returns a label corresponding to the language
For example it returns an array of 0s for af
Return: A nx1 array as integers containing index of the specified language in the "languages" array
'''
def getLabels(lang='de', length=10):
  return np.full((length), languages.index(lang))
# print(getLabels())

'''
Returns all the laguages and labels after reading it from the file
Returns the validation data if train is false and the train data otherwise.
You may assume that the files exist in baseDir and have the same names.
Return: X, y where X is nx1 and y is nx1
'''
def readData(baseDir = '/content/gdrive/My Drive/cis530_hw6/', train=True):
  X = np.array([])
  y = np.array([])
  for lang in languages:
    words = getWords(baseDir=baseDir, lang=lang, train=train)
    X = np.concatenate((X, words))
    y = np.concatenate((y, getLabels(lang=lang, length=words.shape[0])))
  return X, y


'''
Convert a line/word to a pytorch tensor of numbers
Refer the tutorial in the spec
Return: A tensor corresponding to the given line
'''
def line_to_tensor(line):
  tensor = torch.zeros(len(line), 1, len(all_letters))
  for li, letter in enumerate(line):
      tensor[li][0][all_letters.find(letter)] = 1
  return tensor

'''
Returns the category/class of the output from the neural network
Input: Output of the neural networks (class probabilities)
Return: A tuple with (language, language_index)
        language: "af", "cn", etc.
        language_index: 0, 1, etc.
'''
def category_from_output(output):
    top_n, top_i = output.topk(1)
    category_i = top_i[0].item()
    return languages[category_i], category_i
  

'''
Get a random input output pair to be used for training 
Refer the tutorial in the spec
'''

def random_training_pair(X, y):
  num = random.randint(0, y.shape[0] - 1)
  category = y[num]
  line = X[num]
  category_tensor = torch.tensor([category], dtype=torch.long)
  line_tensor = line_to_tensor(line)
  return category, line, category_tensor, line_tensor



# X, y = readData()
# for i in range(10):
#     category, line, category_tensor, line_tensor = random_training_pair(X, y)
#     print('category =', category, '/ line =', line)

'''
Input: trained model, a list of words, a list of class labels as integers
Output: a list of class labels as integers
'''
def predict(model, X_test):
  result = []
  hidden = model.init_hidden()
  for word in X_test:
    line_tensor = line_to_tensor(word)
    for i in range(line_tensor.size()[0]):
      output, hidden = model(line_tensor[i], hidden)
    result.append(category_from_output(output)[1])

  return np.array(result)
  

'''
Input: trained model, a list of words, a list of class labels as integers
Output: The accuracy of the given model on the given input X and target y
'''
def calculateAccuracy(model, X_test, y_test):
  return accuracy_score(y_test, predict(model, X_test))

'''
Train the model for one epoch/one training word.
Ensure that it runs within 3 seconds.
Input: X and y are lists of words as strings and classes as integers respectively
Returns: You may return anything
'''
learning_rate = 0.002
def trainOneEpoch(model, criterion, X, y):
  hidden = model.init_hidden()
  _, _, category_tensor, line_tensor = random_training_pair(X, y)
  model.zero_grad()
  for i in range(line_tensor.size()[0]):
      output, hidden = model(line_tensor[i], hidden)

  loss = criterion(output, category_tensor)
  loss.backward()

  # Add parameters' gradients to their values, multiplied by learning rate
  for p in model.parameters():
      p.data.add_(p.grad.data, alpha=-learning_rate)

  return output, loss.item()


X, y = readData()
# model = CharRNNClassify()
# criterion = nn.NLLLoss()
# print(trainOneEpoch(model, criterion, X, y))

'''
Use this to train and save your classification model. 
Save your model with the filename "model_classify"
'''
n_iters = 100000

X_test, y_test = readData(train=False)


def run():
  current_loss = 0
  model = CharRNNClassify()
  criterion = nn.NLLLoss()
  for iter in tq.tqdm(range(1, n_iters + 1)):
    output, loss = trainOneEpoch(model, criterion, X, y)
    current_loss += loss

    # Print iter number, loss, name and guess
    if iter % 1000 == 0:
      print('[%d] loss: %.3f' % (iter, current_loss))
      current_loss = 0.0
  print(calculateAccuracy(model, X_test, y_test))
  # torch.save(model.state_dict(), PATH)

run()

#models.py
import torch.nn as nn
from torch.autograd import Variable
import torch
import torch.nn.functional as F

'''
Please add default values for all the parameters of __init__.
'''
class CharRNNClassify(nn.Module):
    def __init__(self, input_size=57, hidden_size=128, output_size=9):
      super(CharRNNClassify, self).__init__()
      self.hidden_size = hidden_size
      self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
      self.i2o = nn.Linear(input_size + hidden_size, output_size)
      self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, input, hidden=None):
      combined = torch.cat((input, hidden), 1)
      hidden = self.i2h(combined)
      output = self.i2o(combined)
      output = self.softmax(output)
      return output, hidden

    def init_hidden(self):
      return torch.zeros(1, self.hidden_size)